Monitoring and Quality Assurance Plan
This document outlines the strategy and mechanisms for monitoring the pipeline's performance, estimating its operational cost, and validating the quality of its output.

1. Performance Monitoring
Performance is measured by observing system resource utilization and total execution time. The goal is to establish a baseline and track performance trends, not to meet arbitrary targets.

Key Metrics
Execution Duration: The total wall-clock time required for the entire pipeline and for each individual stage to complete.

CPU Utilization: The percentage of CPU resources consumed during execution.

Memory (RAM) Usage: The amount of memory consumed by the pipeline's processes.

Disk I/O: The rate of read/write operations, particularly during Stage 1.

Implementation
Execution Time: The master run_pipeline.sh orchestrator script is the primary tool for performance monitoring. It logs start and end timestamps for the entire pipeline and for each stage, providing a detailed breakdown of execution duration in its timestamped log files.

Resource Utilization: For detailed analysis during development or troubleshooting, standard system utilities like top (on macOS) or htop should be used to monitor the resource consumption of the Python processes in real-time.

2. Cost Monitoring & Estimation
Cost is monitored by tracking API calls and applying a predefined cost model. The goal is to provide a reliable, transparent estimate for each pipeline run.

Azure Service Cost Model
The following rates are used for cost estimation. These should be periodically updated to reflect current Azure pricing.

Service

Model

Unit

Cost (USD)

Notes

Azure OpenAI

GPT-4o

1 Image (High Res)

$0.00765

Fixed cost for vision analysis

Azure OpenAI

GPT-4o

1,000 Input Tokens (Text)

$0.005

For text cleanup & synthesis

Azure OpenAI

GPT-4o

1,000 Output Tokens (Text)

$0.015

For text cleanup & synthesis

Azure Doc Intelligence

N/A

Per-operation

Varies

Billed via markitdown library usage

Implementation
Automated Estimation: The stage_2_processing.py script is responsible for tracking all API calls made for vision analysis and text cleanup.

Summary Log: Upon completion, Stage 2 generates a machine-readable summary log (_stage2_processing.json) that includes the total number of API calls and a final estimated cost for the stage's operations, based on the model above.

3. Quality Assurance (QA) & Validation
Pipeline success is not only determined by a successful execution (exit code 0), but by the quantifiable quality of the generated artifacts.

Quality Artifacts
The primary sources of truth for QA are the machine-readable logs and reports generated by the pipeline:

preprocessed_markdown/_stage1_processing.json: Confirms successful OCR and image extraction for each file.

final_markdown/_stage2_processing.json: Confirms successful API interaction and enrichment.

markitdown_output/_stage3_processing.json: Confirms successful final synthesis.

markitdown_output/*_quality_report.json: Provides quantitative metrics on the structure and content of the final document.

Validation Protocol
A pipeline run is considered a "quality success" if it meets the following criteria:

Successful Execution: The run_pipeline.sh script completes with an exit code of 0.

No Fatal Errors: The pipeline_logs/errors.log file is empty.

Complete Processing: The number of "successful" files in each stage's summary log matches the total number of input PDFs.

Quantitative Quality Checks: The metrics in each document's _quality_report.json meet or exceed the project's baseline targets. For a typical technical document, this includes:

header_count > 5

code_block_count >= 0

list_item_count > 10

total_characters > 1000

Documents that pass execution but fail the quantitative quality checks should be flagged for manual review.
